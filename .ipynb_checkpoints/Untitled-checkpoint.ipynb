{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b304e1a",
   "metadata": {},
   "source": [
    "# Colon Cancer #\n",
    "### _Predicting the outcomes of colon cells to predict cancer_ ###\n",
    "\n",
    "### Business Understanding ###\n",
    "Colon cancer has been deemed the number 3 most common cancer in the world, according to the World Cancer Research Fund. Based on this statistic, it is not a surprise to know that more approximately 19 million colonoscopies are perfeormed each year in the United States.\n",
    "\n",
    "Some experts believe that some of the main causes of this cancer is the Western food diet along with living a sedentary lifestyle as well as being obese. Unfortunately, according to the CDC, the US appears to be on an upward trend in obesity which in turn increses the likelihood of men and women to develop colorectal cancers.\n",
    "\n",
    "Although the morttality rate for the most part appears to be relatively low (80% survival rate), it is important to note that like everything, there is always something to improve with either accurate test results, the time it takes to report those results and the resources available to compile said results.\n",
    "\n",
    "Currently, as per the American Cancer Society, it takes 2-3 days to report the findings of a colonoscopy biopsy.\n",
    "\n",
    "Objective\n",
    "This notebook has the objective of finding out the population that is deeply affected by colon cancer and build a Convolutional Neural Network that can get close to the 1-2% accuracy that current tests. We will also strive to have an efficient model that can give accurate results faster than 2-3 days and ideally within the time frame of \"same-day\" results.\n",
    "\n",
    "Before doing so, we will look at some mortality rates among different populations and determine whether the economic status of a population affects the mortality rate of colon cancer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9116373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import pathlib\n",
    "# Packages to import and preprocess images\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Packages for our models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPooling2D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd1156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a4495",
   "metadata": {},
   "source": [
    "Deleted all folders from zenodo except NORM which is normal and TUM which is the cancer cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778f70c",
   "metadata": {},
   "source": [
    "Inspiration for the function in the creation of the [directories](https://www.youtube.com/watch?v=_L2uYfVV48I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b87c4b",
   "metadata": {},
   "source": [
    "In order to have class balance in the dataset we will be using a total of 24,000 images for our model training, 1,800 items for the validation, and 1,720 images for our test dataset to generate predictions. This brings our total of images used to 27,520 images used in this Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9649dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('../data/')\n",
    "# if os.path.isdir('train/normal') is False:\n",
    "#     os.makedirs('train/normal')\n",
    "#     os.makedirs('train/cancer')\n",
    "#     os.makedirs('validation/normal')\n",
    "#     os.makedirs('validation/cancer')\n",
    "#     os.makedirs('test/')\n",
    "\n",
    "    \n",
    "#     for image in random.sample(glob.glob('NCT-CRC-HE-100K/NORM/NORM*'), 8000):\n",
    "#         shutil.move(image, 'train/normal')\n",
    "#     for image in random.sample(glob.glob('NCT-CRC-HE-100K/TUM/TUM*'), 8000):\n",
    "#         shutil.move(image, 'train/cancer')\n",
    "#     for image in random.sample(glob.glob('NCT-CRC-HE-100K/NORM/NORM*'), 400):\n",
    "#         shutil.move(image, 'validation/normal')\n",
    "#     for image in random.sample(glob.glob('NCT-CRC-HE-100K/TUM/TUM*'), 400):\n",
    "#         shutil.move(image, 'validation/cancer')\n",
    "#     for image in random.sample(glob.glob('NCT-CRC-HE-100K/NORM/NORM*'), 360):\n",
    "#         shutil.move(image, 'test/normal')\n",
    "#     for image in random.sample(glob.glob('NCT-CRC-HE-100K/TUM/TUM*'), 360):\n",
    "#         shutil.move(image, 'test/cancer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    zoom_range = (0.95,0.95),\n",
    "    brightness_range = [0.5, 1.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606325f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_gen.flow_from_directory(\n",
    "    '../data/train',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 20,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = True,\n",
    "    class_mode = 'binary',\n",
    "    subset = 'training',\n",
    "    seed = 20\n",
    ")\n",
    "validation_generator = data_gen.flow_from_directory(\n",
    "    '../data/validation',\n",
    "    target_size = (224,224),\n",
    "    batch_size = 20,\n",
    "    color_mode = 'rgb',\n",
    "    shuffle = True,\n",
    "    class_mode = 'binary',\n",
    "    subset = 'training',\n",
    "    seed = 20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../data/train/'\n",
    "validation_path = '../data/validation/'\n",
    "test_hold_path = '../data/test/'\n",
    "categories = ['cancer', 'normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7617065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in categories:\n",
    "    path_train = os.path.join(train_path,category)\n",
    "    for img in os.listdir(path_train):\n",
    "        img_array_train = cv2.imread(os.path.join(path_train,img))\n",
    "        plt.imshow(img_array_train)\n",
    "        plt.show()\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e97e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "validation_data = []\n",
    "test_data = []\n",
    "\n",
    "def create_training_data():\n",
    "    \"\"\"\n",
    "    Using a for loop, the images in data_dir_train and the labels in category are appended to train_data to\n",
    "    create our training data set.\n",
    "    \"\"\"\n",
    "    for category in categories:\n",
    "        path_train = os.path.join(train_path,category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path_train):\n",
    "            img_array_train = cv2.imread(os.path.join(path_train,img))\n",
    "            train_array = cv2.resize(img_array_train,(224, 224))\n",
    "            train_data.append([train_array, class_num])\n",
    "            \n",
    "def create_validation_data():\n",
    "    \"\"\"\n",
    "    Using a for loop, the images in data_dir_test and the labels in category are appended to test_data to\n",
    "    create our testing data set.\n",
    "    \"\"\"\n",
    "    for category in categories:\n",
    "        path_test = os.path.join(validation_path,category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path_test):\n",
    "            img_array_test = cv2.imread(os.path.join(path_test,img))\n",
    "            test_array = cv2.resize(img_array_test,(224, 224))\n",
    "            validation_data.append([test_array, class_num])\n",
    "            \n",
    "            \n",
    "def create_testing_data():\n",
    "    \"\"\"\n",
    "    Using a for loop, the images in data_dir_test and the labels in category are appended to test_data to\n",
    "    create our testing data set.\n",
    "    \"\"\"\n",
    "    for category in categories:\n",
    "        path_test = os.path.join(test_hold_path,category)\n",
    "        class_num = categories.index(category)\n",
    "        for img in os.listdir(path_test):\n",
    "            img_array_test = cv2.imread(os.path.join(path_test,img))\n",
    "            test_array = cv2.resize(img_array_test,(224, 224))\n",
    "            test_data.append([test_array, class_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06fd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_data()\n",
    "create_validation_data()\n",
    "create_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73446ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642be480",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eda9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2d440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20)\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(validation_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ecb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in train_data[:1]:\n",
    "    print('Train array', sample[0])\n",
    "    print('Train label', sample[1])\n",
    "    \n",
    "    \n",
    "for sample in validation_data[:1]:\n",
    "    print('Validation array', sample[0])\n",
    "    print('Validation label', sample[1])\n",
    "    \n",
    "for sample in test_data[:1]:\n",
    "    print('Testing array', sample[0])\n",
    "    print('Testing label', sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid, X_test, y_test = ([] for list in range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e612f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_label_from_array(array, x, y):\n",
    "    \"\"\"\n",
    "    Separates the features and the label from a specified array and appends it to specified X list and y list.\n",
    "    \"\"\"\n",
    "    for features, label in array:\n",
    "        x.append(features)\n",
    "        y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d727cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features_label_from_array(train_data, X_train, y_train)\n",
    "create_features_label_from_array(validation_data, X_valid, y_valid)\n",
    "create_features_label_from_array(test_data, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be898cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59f3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_valid = np.array(X_valid)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b3c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training shape: ', X_train.shape)\n",
    "print('Validation shape: ', X_valid.shape)\n",
    "print('Testing shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadbbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255.0\n",
    "X_valid = X_valid/255.0\n",
    "X_test = X_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d92f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
    "y_valid =tf.keras.utils.to_categorical(y_valid, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b67869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training shape: ', y_train.shape)\n",
    "print('Validation shape: ', y_valid.shape)\n",
    "print('Testing shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91284c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6eb164b",
   "metadata": {},
   "source": [
    "# Model 1 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='tanh', padding='same', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(2, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b024ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d56d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414194a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_generator,\n",
    "          validation_data = validation_generator,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6af51a",
   "metadata": {},
   "source": [
    "As per the documentation of the dataset it appears that the images in the zenodo dataset are 224x224 pixels. The dataset in the kaggle dataset are 768x768. IN this case we will make the 224x224 size standard accross all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=train_path, \n",
    "                                                                             target_size=(224,224), \n",
    "                                                                             classes=['cancer', 'normal'], \n",
    "                                                                             batch_size=100)\n",
    "validation_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=validation_path,\n",
    "                                                                                  target_size=(224,224), \n",
    "                                                                                  classes=['cancer', 'normal'], \n",
    "                                                                                  batch_size=100)\n",
    "test_dataset_batch = ImageDataGenerator(rescale=1./255).flow_from_directory(directory=test_hold_path,\n",
    "                                                                            target_size=(224,224), \n",
    "                                                                            classes=['cancer', 'normal'], \n",
    "                                                                            batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b897e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train, label_train = next(train_dataset_batch)\n",
    "images_validation, label_validation = next(validation_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc93207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img):\n",
    "    fig, axes = plt.subplots(1,10, figsize=(10,10))\n",
    "    axes = axes.flatten()\n",
    "    for image, ax in zip(img, axes):\n",
    "        ax.imshow(image)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e3891",
   "metadata": {},
   "source": [
    "Based on the order of how we called the classes when defining our batches, [1,0] refers to a normal cell and [0,1] refers to a cancer cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd828a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images_train.shape)\n",
    "print(images_validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e91065",
   "metadata": {},
   "source": [
    "## Build the first CNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e626a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), activation='tanh', padding='same', input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "model.add(Dense(2, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9df4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9861aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=train_dataset_batch,\n",
    "          validation_data = (validation_dataset_batch),\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28416a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa096284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e95b82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
